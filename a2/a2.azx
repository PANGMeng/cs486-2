$import aztex-lib/latex.azx
$import aztex-lib/amsmath.azx
$import aztex-lib/graphicx.azx
$import aztex-lib/listings.azx

$titlepage{CS 486 - Assignment 2}{Alex Klen \\ 20372654}

@{
  $section{Written Questions} {
    $enumerateN{a.$rparen}{
      $item{ % a
        The algorithm will explore the states in the following order: #{s, h, k, c, a, b, d, m, e, n, g}.
      }
      $item{ % b
        $enumerateN{i.$rparen}{
          $item{ % i
            #h is admissible because it satisfies the following two criteria. #{h(g) = 0} because the goal is zero steps from itself. For Manhattan Distance, #{h(n) $le h^*(n)} (a heuristic will give a cost from node #n to the goal of at most the cost of the shortest path) because the Manhattan Distance gives the shortest possible number of moves. This is proven below. $nl
            Suppose we have a path #p from #n to #g where #{|p| < h(n)}. #p needs to have at least #{dx = |g.x - n.x|} horizontal moves and at least #{dy = |g.y - n.y|} vertical moves, so #{|p| $ge dx + dy}. But Manhattan Distance is exactly this quantity: #{h(n) = dx + dy}. Then #{|p| $ge dx + dy = h(n)}, a contradiction. $nl
            Therefore, #h is an admissible heuristic function.
          }
          $item{ % ii
            The algorithm will explore the states in the following order: #{s, h, k, c, a, b, d, m, g}. $nl
          }
          $item{ % iii
            The algorithm will explore the states in the following order: #{s, h, k, c, f, p, q, r, t, g}. $nl
            Note that the algorithm might not explore #c , since at that step #c and #f have the same cost plus heuristic value.
          }
        }
      }
    }
  }

  $section{Programming Questions}{
    $enumerateN{A.}{
      $item{ % A
        $bold{Informed Search} $nl
        $enumerateN{a.}{
          $item{ % a
            A state is a partial tour, represented by AStarNode in my implementation. In this case the path A* takes isn't needed to compute the cost of each state, because this information is stored in the state. The reason states need this information is because the heuristic needs to know which cities the tour has visited so far, so a state needs to include the partial tour. $nl$nl
            The initial state is simply a partial tour of only city A. $nl$nl
            Goal states are any states that has a tour of every city - the length of the tour is the number of cities in the problem. $nl$nl
            The operator to get to new states is simply to choose each city not in the current state's tour and append it to the tour. $nl$nl
            The cost of a state is simply the sum of distances of consecutive cities. If the tour is partial, the cost does not include a return edge to the start node. If it is complete, however, it is the full cost of the cycle.
          }
          $item{ % b
            My heuristic function for an incomplete tour is as follows:
            $math{
              & @{Let path} = [p_1, ..., p_k] \\
              & @{Let unvisited} = $braces@cities - $braces@path \\
              & h(@path) = @dist(@MST(@unvisited)) + $Min{n $in @unvisited}{@dist(p_k, n)} + $Min{n $in @unvisited}{@dist(n, p_1)} \\
            }
            The heuristic function is 0 for a complete tour. I did not come up with this heuristic, I found it on page 26 of $it{Heuristic Search: Theory and Applications} By Stefan Edelkamp, Stefan Schroedl. $nl
            It is admissible because of the following observation. The optimal tour #{T^* = [p_1..p_k, u_1, u_l, p_1]} has some path through the remaining unvisited nodes #{u_1..u_l}, with the edges #(p_k, u_1) and #(u_l, p_1). The path #[u_1..u_l] is a spanning tree of the unvisited nodes. By definition the minimum spanning tree (MST) has sum of edge lengths less than or equal to any spanning tree. $nl
            $math{
              @dist(T^*) &= @dist(p_1..p_k, u_1..u_l, p_1) \\
                         &= @dist(p_1..p_k) + @dist(p_k, u_1) + @dist(u_1..u_l) + @dist(u_l, p_1) \\
                         &$le @dist(p_1..p_k) + $Min{n $in @unvisited}{@dist(p_k, n)} + @dist(@MST(@unvisited)) + $Min{n $in @unvisited}{@dist(n, p_1)} \\
                         &= @dist(p_1..p_k) + h(@path) \\
              $therefore @{ } @dist(T^*) &- @dist(p_1..p_k) $le h(@path) \\
            }
          }
          $item{ % c
            The following graph shows the number of A* states created and placed in the priority queue during the search averaged over all 10 given TSPs.
            The vertical axis uses a logarithmic scale and an exponential trend line is shown. $nl
            $graphic{astar_performance.png}{scale=1.0} $nl
            The trend line shows the approximate number of states generated for a 36-city TSP, which is 36.3 million. $nl
            The chart below shows the averaged running times for the same runs. $nl
            $graphic{astar_runtime.png}{scale=1.0} $nl
            The trend line shows the approximate runtime a 36-city TSP would be $approx 18 hours. $nl
          }
        }
        $bold{A* Code} $nl
        First the TSP representation, generic search code, and the main program. I implemented this assignment in Haskell.
        $nl
        $codeFile{Haskell}{TSP.hs}
        $codeFile{Haskell}{Search.hs}
        $codeFile{Haskell}{Main.hs}
        $nl
        Following is a generic A* implementation and the code that applies it to solve TSP.
        $nl
        $codeFile{Haskell}{AStar.hs}
        $codeFile{Haskell}{TSPAStar.hs}
      }
      $item{ % B
        $bold{Local Search} $nl
        $enumerateN{a.}{
          $item{ % a
            My neighbourhood operator is to swap the endpoints of two edges in an existing tour. This is the same as reversing a range in a permutation of cities. More concretely, the neighbours for #{T = [p_1..p_n, p_1]} are defined as follows: $nl
            $let i = $hspace 5pt
            $math{
              &@neighbours = $braces{} \\
              &@{for } i <- [1 .. n-3] \\
              &$i@{for } j <- [i+1 .. n-2] \\
              &$i$i@neighbours = @neighbours $union $braces{[p_1..p$un{i-1}, p_j, p$un{j-1}..p$un{i+1}, p_i, p$un{j+1}.. p_n]} \\
            }
          }
          $item{ % b
            The three cooling schedules I used are shown below. These are scaled to run for 1000 time steps.
            $math{
              &@{1. Linear: } &&5*(1000 - x) \\
              &@{2. Logarithmic: } &&5*(300*$log(-x/100 + 30)) \\
              &@{3. Oscillatory: } &&5*(501 - x/4 + (1000 - x/2)/2 * $cos(x/5) * $sin(x/12)) \\
            }
            I think using some form of oscillating schedule would be best, although the formula above isn't necessarily a great one. The idea is that the algorithm will be in an exploitation phase when the temperature drops and it will find a local minimum, and then as the temperature rises it will be in an exploration phase where it can escape valleys and pass into other ones.
          }
          $item{ % c
            $graphic{sa_performance.png}{width=\textwidth} $nl
            The cost of the best solution found was TEMP. One thing to note is that the linear cooling schedule quickly finds a local minimum and then never finds a better solution after that because it cannot escape a local minimum. The logarithmic schedule is at a higher temperature for a longer period of time and so makes progress later than the linear one, and the oscillatory schedule makes some progress even later.
          }
          $item{ % d
            Simulated Annealing is complete, assuming that you start at a goal state and your operator moves to other valid goal states. Since it only looks at goal stats it will output a goal state after running, but it may not be optimal.
          }
          $item{ % e
            Simulated Annealing is not optimal because it doesn't
          }
        }
      }
    }
    $bold{Simulated Annealing Code} $nl
    Following is a generic Simulated Annealing implementation and the code that applies it to solve TSP.
    $nl
    $codeFile{Haskell}{SimulatedAnnealing.hs}
    $codeFile{Haskell}{TSPSimulatedAnnealing.hs}
  }
}


